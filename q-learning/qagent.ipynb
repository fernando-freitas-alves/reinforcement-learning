{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gym\n",
    "from gym import logger\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clear_function():\n",
    "    try:\n",
    "        __IPYTHON__\n",
    "        return lambda: clear_output(wait=True)\n",
    "    except NameError:\n",
    "        if os.name == 'nt':\n",
    "            return lambda: os.system('cls')\n",
    "        else:\n",
    "            return lambda: os.system('clear')\n",
    "\n",
    "clear = clear_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration: int, total: int, prefix: str = '', suffix: str = '', decimals: int = 1, length=100, fill: str = '█', unfill: str = '-', min_filled_length: int = 0, print_percent: bool = True, print_end: str = os.linesep):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        print_end   - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = round(100 * iteration / total, decimals)\n",
    "    filled_length = max(int(length * iteration // total), min_filled_length)\n",
    "    bar = fill * filled_length + unfill * (length - filled_length)\n",
    "    print(f'\\r{prefix} [{bar}]', end='')\n",
    "    if print_percent:\n",
    "        print(f' {percent}%', end='')\n",
    "    print(f' {suffix}', end=print_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, observation_space: gym.Space, action_space: gym.Space):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def _print_details_(self, *_) -> None:\n",
    "        pass\n",
    "\n",
    "    def _print_progress_bar_(self, *_) -> None:\n",
    "        pass\n",
    "\n",
    "    def step(self, *_) -> np.array:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, *_) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, observation_space: gym.Space, action_space: gym.Space):\n",
    "        super().__init__(observation_space, action_space)\n",
    "\n",
    "    def step(self, *_) -> np.array:\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DiscreteAgent(Agent):\n",
    "    def __init__(self, observation_space: gym.Space, action_space: gym.Space, discretization_size: list = [20, 20]):\n",
    "        super().__init__(observation_space, action_space)\n",
    "        self.observation_size, self.observation_step = self._discretize_space_(observation_space, discretization_size[0])\n",
    "        self.action_size,      self.action_step      = self._discretize_space_(action_space, discretization_size[1])\n",
    "\n",
    "    def _discretize_space_(self, space: gym.Space, discretization_size: list) -> list:\n",
    "        if type(space) is gym.spaces.discrete.Discrete:\n",
    "            size = [space.n]\n",
    "            discretization_step = None\n",
    "        else:\n",
    "            size = np.array([discretization_size] * np.prod(space.shape))\n",
    "            discretization_step = (space.high - space.low) / (size - 1)\n",
    "        return tuple(size), discretization_step\n",
    "\n",
    "    def _get_continuous_action_(self, action: np.array) -> Tuple:\n",
    "        return self._get_continuous_space_value_(action, self.action_space, self.action_step)\n",
    "\n",
    "    def _get_continuous_observation_(self, observation: np.array) -> Tuple:\n",
    "        return self._get_continuous_space_value_(observation, self.observation_space, self.observation_step)\n",
    "\n",
    "    def _get_continuous_space_value_(self, discrete_space_value: Tuple, space: gym.Space, discretization_step: np.array) -> Tuple:\n",
    "        if type(space) is gym.spaces.discrete.Discrete:\n",
    "            space_value = np.asscalar(discrete_space_value)\n",
    "        else:\n",
    "            space_value = np.array(space.low + discrete_space_value * discretization_step)\n",
    "        return space_value\n",
    "\n",
    "    def _get_discrete_action_(self, action: np.array) -> Tuple:\n",
    "        return self._get_discrete_space_value_(action, self.action_space, self.action_step)\n",
    "\n",
    "    def _get_discrete_observation_(self, observation: np.array) -> Tuple:\n",
    "        return self._get_discrete_space_value_(observation, self.observation_space, self.observation_step)\n",
    "\n",
    "    def _get_discrete_space_value_(self, space_value: np.array, space: gym.Space, discretization_step: np.array) -> Tuple:\n",
    "        if type(space) is gym.spaces.discrete.Discrete:\n",
    "            discrete_space_value = (space_value,)\n",
    "        else:\n",
    "            discrete_space_value = ((space_value - space.low) / discretization_step).astype(np.int)\n",
    "        return tuple(discrete_space_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class QAgent(DiscreteAgent, RandomAgent):\n",
    "    def __init__(self, observation_space: gym.Space, action_space: gym.Space, discount_factor: float = 0.95, learning_rate: float = 0.01, exploration_decay_rate: float = 0.99, min_exploration_rate = 0.1, discretization_size: list = [20, 20]):\n",
    "        super().__init__(observation_space, action_space, discretization_size)\n",
    "        self.discount_factor        = discount_factor\n",
    "        self.learning_rate          = learning_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.min_exploration_rate   = min_exploration_rate\n",
    "        self.exploration_rate       = 1\n",
    "        self.q_table                = np.zeros(self.observation_size + self.action_size)\n",
    "\n",
    "    def step(self, observation: np.array,ep) -> np.array:\n",
    "        discrete_observation = self._get_discrete_observation_(observation)\n",
    "        noisy_q_row          = self.q_table[discrete_observation] + self.exploration_rate * np.random.randn(*self.action_size)\n",
    "        discrete_action      = np.argmax(noisy_q_row)\n",
    "        action               = self._get_continuous_action_(discrete_action)\n",
    "        return action\n",
    "\n",
    "    def train(self, observation: np.array, action: np.array, next_observation: np.array, reward: float, done: bool, goal_reached: bool, batch_goals_ratio: float) -> None:\n",
    "        # Discretize spaces\n",
    "        discrete_observation      = self._get_discrete_observation_(observation)\n",
    "        discrete_action           = self._get_discrete_action_(action)\n",
    "        next_discrete_observation = self._get_discrete_observation_(next_observation)\n",
    "        # Learning rule\n",
    "        q_best_next = np.max(self.q_table[next_discrete_observation])\n",
    "        q_objective = reward + self.discount_factor * q_best_next  # Bellman's equation approximation\n",
    "        error       = q_objective - self.q_table[discrete_observation + discrete_action]\n",
    "        self.q_table[discrete_observation + discrete_action] += self.learning_rate * error\n",
    "        if done:\n",
    "            min_exploration_rate  = (1 - batch_goals_ratio) * self.min_exploration_rate\n",
    "            self.exploration_rate = max(0, self.exploration_rate * self.exploration_decay_rate, min_exploration_rate)\n",
    "            self.exploration_rate = min(1, self.exploration_rate)\n",
    "\n",
    "    def _print_details_(self) -> None:\n",
    "        print(f'Exploration rate = {self.exploration_rate}')\n",
    "        if np.prod(self.q_table.shape) < 200:\n",
    "            print('Q-table:')\n",
    "            print(self.q_table)\n",
    "\n",
    "    def _print_progress_bar_(self) -> None:\n",
    "        print_progress_bar(1 - self.exploration_rate, 1, prefix='Exploitation ',\n",
    "                           suffix=f'Exploration (rate = {round(self.exploration_rate, 3)})', length=50, fill='█', unfill='░', min_filled_length=1, print_percent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "    def __init__(self, environment: str = 'CartPole-v0', agent: Agent = RandomAgent, is_goal_reached: Callable[[np.array, gym.Env, float, bool], bool] = lambda state, environment, reward, done: False, output_filename : str = [], output_size : str = 'double', plot_results : bool = True, logger_level: int = logger.INFO):\n",
    "        logger.set_level(logger_level)\n",
    "        if len(output_filename) == 0:\n",
    "            output_filename = 'results_' + environment + '.png'\n",
    "        self.output_filename = output_filename\n",
    "        self.output_size     = output_size\n",
    "        self.plot_results    = plot_results\n",
    "        self.environment     = gym.make(environment)\n",
    "        self.agent           = agent(self.environment.observation_space,\n",
    "                                     self.environment.action_space)\n",
    "        self.is_goal_reached = is_goal_reached\n",
    "        self.environment.seed(0)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.environment.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def _new_statistic_value_(self):\n",
    "        return {'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "    def _print_details_(self):\n",
    "        clear()\n",
    "        if self.render:\n",
    "            self.environment.render()\n",
    "            print()\n",
    "        print(f'Episode: {self.episode}, Reward: {self.reward}, Episode reward: {self.episode_reward}, Bach goals reached: {self.goals_reached} / {self.render_batch_size}')\n",
    "        self.agent._print_details_()\n",
    "        if self.done:\n",
    "            time.sleep(10*(self.num_episodes - self.episode) / self.num_episodes)\n",
    "        else:\n",
    "            time.sleep(self.print_sleep_sec)\n",
    "\n",
    "    def _print_progress_bars_(self):\n",
    "        clear()\n",
    "        print_progress_bar(self.render_batch, self.max_render_batches, prefix='Batches      ',\n",
    "                           suffix=f'({self.render_batch} / {self.max_render_batches} total)', length=50)\n",
    "        print_progress_bar(self.batch_episodes, self.render_batch_size, prefix='Episodes     ',\n",
    "                           suffix=f'batch of {self.render_batch_size} ({self.episode} / {self.num_episodes} total)', length=50)\n",
    "        print_progress_bar(self.goals_reached, self.render_batch_size, prefix='Goals reached',\n",
    "                           suffix=f'batch of {self.render_batch_size} ({self.total_goals_reached} / {self.num_episodes} total)', length=50)\n",
    "        self.agent._print_progress_bar_()\n",
    "\n",
    "    def _plot_results_(self, x = [], y = [], xlim = [], ylim = [], xlabel = [], ylabel = [], color = []):\n",
    "        def plot_value_statistics(ax, x, y, label = '', color = 'k', zorder = 1):\n",
    "            ax.fill_between(x, y['min'], y['max'], linewidth=0, color=color, alpha=0.2, zorder=zorder)\n",
    "            ax.plot(x, y['avg'], label=label, color=color, zorder=zorder)\n",
    "        def plot(x, y, xlim = [], ylim = [], xlabel = '', ylabel = [], color = [], size = 'simple'):\n",
    "            if len(ylabel) == 0:\n",
    "                ylabel = [''] * len(y)\n",
    "            if len(color) == 0:\n",
    "                color = ['k'] * len(y)\n",
    "            size = size.lower()\n",
    "            lim  = lambda A: (min(A), max(A))\n",
    "            if len(xlim) == 0:\n",
    "                xlim = lim(x)\n",
    "            if len(ylim) == 0:\n",
    "                ylim = [xlim] + \\\n",
    "                       [lim(yi['avg'] + yi['min'] + yi['max'])\n",
    "                            if isinstance(yi, dict) \n",
    "                            else lim(yi) for yi in y[1:]]\n",
    "            plot_fnc = lambda ax, x, y, *args, **kwargs: \\\n",
    "                            plot_value_statistics(ax, x, y, *args, **kwargs) \\\n",
    "                            if isinstance(y, dict) else \\\n",
    "                            ax.plot(x, y, *args, **kwargs)\n",
    "            # matplotlib.use(\"pgf\")\n",
    "            matplotlib.rcParams.update({\n",
    "                'font.family'  : 'Times New Roman',\n",
    "                'font.size'    : '8',\n",
    "                'text.usetex'  : True,\n",
    "                'pgf.texsystem': \"pdflatex\",\n",
    "                'pgf.rcfonts'  : False\n",
    "            })\n",
    "            paper = {\n",
    "                'width':  8.5,    # in\n",
    "                'height': 11,     # in\n",
    "                'colsep': 5 / 72, # in\n",
    "                'margin': {\n",
    "                    'left'  : 0.75, # in\n",
    "                    'right' : 0.75, # in\n",
    "                    'top'   : 1,    # in\n",
    "                    'bottom': 1     # in\n",
    "                }\n",
    "            }\n",
    "            content = {\n",
    "                'width':  paper['width']  - paper['margin']['left'] - paper['margin']['right'] - paper['colsep'],\n",
    "                'height': paper['height'] - paper['margin']['top']  - paper['margin']['bottom']\n",
    "            }\n",
    "            fig, ax0 = plt.subplots()\n",
    "            if size == 'simple':\n",
    "                fig.set_size_inches(w=content['width'] / 2, h=content['height'] / 4)\n",
    "            elif size == 'double':\n",
    "                fig.set_size_inches(w=content['width'] + paper['colsep'], h=content['height'] / 4)\n",
    "            else:\n",
    "                raise Exception('Figure size unknown')\n",
    "            zorder = len(y)\n",
    "            ax0.set_xlim(*xlim)\n",
    "            ax0.set_ylim(*ylim[0])\n",
    "            ax0.set_xlabel(xlabel)\n",
    "            ax0.set_ylabel(ylabel[0], color=color[0])\n",
    "            ax0.tick_params(axis='y', labelcolor=color[0])\n",
    "            ax0.set_zorder(zorder)\n",
    "            ax0.patch.set_visible(False)\n",
    "            plt.setp(ax0.spines.values(), linewidth=0.5)\n",
    "            plot_fnc(ax0, x, y[0], label=ylabel[0],  color=color[0])#, zorder=zorder)\n",
    "            tight_bbox0  = ax0.get_tightbbox(fig.canvas.get_renderer())\n",
    "            window_bbox0 = ax0.get_window_extent(fig.canvas.get_renderer())\n",
    "            bbox0_label_width = (tight_bbox0.width - window_bbox0.width)\n",
    "            for i in range(1, len(y)):\n",
    "                zorder -= 1\n",
    "                new_ax = ax0.twinx()\n",
    "                new_ax.spines['right'].set_position(('outward', bbox0_label_width*(i-1)))\n",
    "                new_ax.set_ylim(*ylim[i])\n",
    "                new_ax.set_ylabel(ylabel[i], color=color[i])\n",
    "                new_ax.tick_params(axis='y', labelcolor=color[i])\n",
    "                new_ax.set_zorder(zorder)\n",
    "                plt.setp(new_ax.spines.values(), linewidth=0.01)\n",
    "                plot_fnc(new_ax, x, y[i], label=ylabel[i],  color=color[i])#, zorder=zorder)\n",
    "            return fig\n",
    "        if len(x) == 0:\n",
    "            x =  self.statistics['episode']\n",
    "        if len(y) == 0:\n",
    "            y = (self.statistics['goals_reached'],\n",
    "                 self.statistics['reward'])\n",
    "        if len(xlabel) == 0:\n",
    "            xlabel = 'Episode'\n",
    "        if len(ylabel) == 0:\n",
    "            ylabel = ('Cumulative goals reached',\n",
    "                    'Rewards')\n",
    "        if len(color) == 0:\n",
    "            color = ('#0880AB',\n",
    "                     '#CC4F1B')\n",
    "        fig = plot(x, y, xlim, ylim, xlabel, ylabel, color, size=self.output_size)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(self.output_filename, dpi=300)\n",
    "\n",
    "    def _reset_episode_(self):\n",
    "        self.state = self.environment.reset()\n",
    "        self.done = False\n",
    "        self.trajectory = ()\n",
    "        self.episode_reward = 0\n",
    "        return self.state, self.done\n",
    "\n",
    "    def _reset_run_(self, num_episodes = 1000, stat_batch_size = 100, render_batch_size = 100, render = True, print_details = False, plot_results = False, print_sleep_sec = 0, output_filename = [], output_size = []):\n",
    "        self.num_episodes           = num_episodes\n",
    "        self.stat_batch_size        = stat_batch_size\n",
    "        self.render_batch_size      = render_batch_size\n",
    "        self.render                 = render\n",
    "        self.print_details          = print_details\n",
    "        self.plot_results           = plot_results\n",
    "        self.print_sleep_sec        = print_sleep_sec\n",
    "        if len(output_filename) != 0:\n",
    "            self.output_filename = output_filename\n",
    "        if len(output_size) != 0:\n",
    "            self.output_size = output_size\n",
    "        self.rewards             = []\n",
    "        self.total_goals_reached = 0\n",
    "        self.goals_reached       = 0\n",
    "        self.batch_goals_ratio   = 0\n",
    "        self.render_batch        = 0\n",
    "        self.batch_episodes      = 0\n",
    "        self.max_render_batches  = math.ceil(num_episodes / render_batch_size)\n",
    "        self.statistics          = {'episode': [], 'reward': self._new_statistic_value_(), 'goals_reached': []}\n",
    "\n",
    "    def _save_step_(self, episode, state, action, next_state, reward, done, goal_reached):\n",
    "        self.episode      = episode\n",
    "        self.state        = state\n",
    "        self.action       = action\n",
    "        self.next_state   = next_state\n",
    "        self.reward       = reward\n",
    "        self.done         = done\n",
    "        self.goal_reached = goal_reached\n",
    "\n",
    "    def _terminate_episode_(self):\n",
    "        self.rewards.append(self.episode_reward)\n",
    "        self._update_statistics_batch_()\n",
    "        self._update_render_batch_()\n",
    "        if self.goal_reached:\n",
    "            self.goals_reached += 1\n",
    "            self.total_goals_reached += 1\n",
    "        self.batch_episodes += 1\n",
    "        self._print_progress_bars_()\n",
    "\n",
    "    def _terminate_run_(self):\n",
    "        self._show_step_()\n",
    "        if self.plot_results:\n",
    "            self._plot_results_()\n",
    "\n",
    "    def _terminate_step_(self, episode, state, action, next_state, reward, done, goal_reached):\n",
    "        self.trajectory     += ((state, action),)\n",
    "        self.episode_reward += reward\n",
    "        self._save_step_(episode, state, action, next_state, reward, done, goal_reached)\n",
    "        self._show_step_()\n",
    "\n",
    "    def _show_step_(self, force : bool = False):\n",
    "        if force or self.episode % self.render_batch_size == 0:\n",
    "            if self.print_details:\n",
    "                self._print_details_()\n",
    "            elif self.render:\n",
    "                self.environment.render()\n",
    "\n",
    "    def _update_statistics_batch_(self):\n",
    "        self.statistics_bach_concluded = self.episode % self.stat_batch_size == 0\n",
    "        if self.statistics_bach_concluded:\n",
    "            batch_rewards = self.rewards[-self.stat_batch_size:]\n",
    "            self.statistics['episode'].append(self.episode)\n",
    "            self._update_value_statistics_('reward', batch_rewards)\n",
    "            self.statistics['goals_reached'].append(self.total_goals_reached)\n",
    "\n",
    "    def _update_render_batch_(self):\n",
    "        self.render_bach_concluded = self.episode % self.render_batch_size == 0\n",
    "        if self.render_bach_concluded:\n",
    "            self.batch_goals_ratio = self.goals_reached / self.render_batch_size\n",
    "            self.batch_episodes = 0\n",
    "            self.goals_reached = 0\n",
    "            self.render_batch += 1\n",
    "\n",
    "    def _update_value_statistics_(self, name, values):\n",
    "        self.statistics[name]['avg'].append(np.average(values))\n",
    "        self.statistics[name]['min'].append(min(values))\n",
    "        self.statistics[name]['max'].append(max(values))\n",
    "\n",
    "    def run(self, num_episodes: int, stat_batch_size: int, render_batch_size: int, render: bool, print_details: bool, print_sleep_sec: float, output_filename: str, output_size: str) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "problems = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RLProblem(Problem):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _plot_results_(self, *args, **kwargs):\n",
    "        x =  self.statistics['episode']\n",
    "        y = (self.statistics['goals_reached'],\n",
    "             self.statistics['reward'],\n",
    "             self.statistics['exploration_rate'])\n",
    "        ylabel = ('Cumulative goals reached',\n",
    "                  'Rewards',\n",
    "                  'Exploration rate')\n",
    "        color = ('#0880AB',\n",
    "                 '#CC4F1B',\n",
    "                 '#00A000')\n",
    "        lim  = lambda A: (min(A), max(A))\n",
    "        xlim = lim(x)\n",
    "        ylim = [xlim] + \\\n",
    "               [lim(yi['avg'] + yi['min'] + yi['max'])\n",
    "                   if isinstance(yi, dict) \n",
    "                   else lim(yi) for yi in y[1:-1]] + \\\n",
    "               [[0, 1]]\n",
    "        super()._plot_results_(y=y, ylim=ylim, ylabel=ylabel, color=color, *args, **kwargs)\n",
    "\n",
    "    def _reset_run_(self, *args, **kwargs):\n",
    "        super()._reset_run_(*args, **kwargs)\n",
    "        self.exploration_rate = []\n",
    "        self.statistics['exploration_rate'] = self._new_statistic_value_()\n",
    "\n",
    "    def _terminate_episode_(self, *args, **kwargs):\n",
    "        self.exploration_rate.append(self.agent.exploration_rate)\n",
    "        super()._terminate_episode_(*args, **kwargs)\n",
    "\n",
    "    def _update_statistics_batch_(self, *args, **kwargs):\n",
    "        super()._update_statistics_batch_(*args, **kwargs)\n",
    "        if self.statistics_bach_concluded:\n",
    "            batch_exploration_rate = self.exploration_rate[-self.stat_batch_size:]\n",
    "            self._update_value_statistics_('exploration_rate', batch_exploration_rate)\n",
    "\n",
    "    def run(self, *args, **kwargs) -> float:\n",
    "        self._reset_run_(*args, **kwargs)\n",
    "        for episode in range(self.num_episodes):\n",
    "            state, done = self._reset_episode_()\n",
    "            step = 0\n",
    "            while not done:\n",
    "                action = self.agent.step(state,episode)\n",
    "                next_state, reward, done, _ = self.environment.step(action)\n",
    "                goal_reached = self.is_goal_reached( next_state, self.environment, reward, done)\n",
    "                self.agent.train(state, action, next_state, reward, done, goal_reached, self.batch_goals_ratio)\n",
    "                self._terminate_step_(episode, state, action, next_state, reward, done, goal_reached)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            self._terminate_episode_()\n",
    "        self._terminate_run_()\n",
    "        return self.episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/the-computer-scientist/OpenAIGym/blob/7be80647e6e090c76f28ea03b7d1ba891db75f2f/QLearningIntro.ipynb\n",
    "# Author: TheComputerScientist\n",
    "# Date:   Oct 15, 2018\n",
    "try:\n",
    "    register(\n",
    "        id                = 'FrozenLakeNoSlip-v0',\n",
    "        entry_point       = 'gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs            = {'map_name': '4x4', 'is_slippery': False},\n",
    "        max_episode_steps = 100,\n",
    "        reward_threshold  = 0.78\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def is_goal_reached_FrozenLakeNoSlip_v0(state, environment, reward, done):\n",
    "    return done and reward == 1\n",
    "\n",
    "problems['FrozenLakeNoSlip-v0'] = RLProblem(environment=\"FrozenLakeNoSlip-v0\", output_filename='doc/img/results_FrozenLakeNoSlip-v0.pgf', is_goal_reached=is_goal_reached_FrozenLakeNoSlip_v0,\n",
    "                                            agent=lambda *args: QAgent(*args, discount_factor=0.99, learning_rate=0.85, exploration_decay_rate=0.99, min_exploration_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_goal_reached_FrozenLake_v0(state, environment, reward, done):\n",
    "    return done and reward == 1\n",
    "\n",
    "problems['FrozenLake-v0'] = RLProblem(environment=\"FrozenLake-v0\", output_filename='doc/img/results_FrozenLake-v0.pgf', is_goal_reached=is_goal_reached_FrozenLake_v0,\n",
    "                                      agent=lambda *args: QAgent(*args, discount_factor=0.99, learning_rate=0.85, exploration_decay_rate=0.99, min_exploration_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_goal_reached_MountainCar_v0(state, environment, reward, done):\n",
    "    return state[0] >= environment.goal_position\n",
    "\n",
    "problems['MountainCar-v0'] = RLProblem(environment=\"MountainCar-v0\", output_filename='results_MountainCar-v0.png', is_goal_reached=is_goal_reached_MountainCar_v0,\n",
    "                                       agent=lambda *args: QAgent(*args, discount_factor=0.99, learning_rate=0.9, exploration_decay_rate=0.99, min_exploration_rate=0.001, discretization_size=[20, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_goal_reached_MountainCarContinuous_v0(state, environment, reward, done):\n",
    "    return state[0] >= environment.goal_position\n",
    "\n",
    "problems['MountainCarContinuous-v0'] = RLProblem(environment=\"MountainCarContinuous-v0\", output_filename='results_MountainCarContinuous-v0.pgf', is_goal_reached=is_goal_reached_MountainCarContinuous_v0,\n",
    "                                                 agent=lambda *args: QAgent(*args, discount_factor=0.99, learning_rate=0.9, exploration_decay_rate=0.99995, min_exploration_rate=0.001, discretization_size=[20, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem = problems['FrozenLakeNoSlip-v0']\n",
    "problem = problems['FrozenLake-v0']\n",
    "# problem = problems['MountainCar-v0']\n",
    "# problem = problems['MountainCarContinuous-v0']\n",
    "\n",
    "# problem.run(num_episodes=1,        render=False, render_batch_size=1,    print_details=False)\n",
    "# problem.run(num_episodes=250,      render=False, render_batch_size=250,  print_details=False)\n",
    "# problem.run(num_episodes=250*10,   render=False, render_batch_size=250,  print_details=False)\n",
    "# problem.run(num_episodes=250*100,  render=True,  render_batch_size=250,  print_details=True, print_sleep_sec=0.5)\n",
    "# problem.run(num_episodes=250*1000, render=False, render_batch_size=250,  print_details=False)\n",
    "\n",
    "problem.run(num_episodes=250*100,  render=True,  render_batch_size=2500, print_details=True, print_sleep_sec=0.125)\n",
    "# problem.run(num_episodes=250*100,  render=True,  render_batch_size=2500, print_details=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.run(num_episodes=1, render=True, render_batch_size=1, print_details=True, print_sleep_sec=0.125, plot_results=False)\n",
    "# problem.run(num_episodes=1, render=True, render_batch_size=1, print_details=False, print_sleep_sec=0.125, plot_results=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python38264bit95c5e6788d2c4977a6ff1288c447bfa0",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}