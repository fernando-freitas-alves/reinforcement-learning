\documentclass[conference]{IEEEtran}

\usepackage{algorithm,algorithmic}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
% \usepackage{multibib}
\usepackage{pgfplots}
\usepackage[mode=buildnew]{standalone}
\usepackage{textcomp}
\usepackage{xcolor}
\pgfplotsset{compat=1.17}

\usepackage{customcommand}

%██████████████████████████████████████████████████████████████████████████████████████████████████

\begin{document}

\title{{\QLearning} Applied to OpenAI Gym}
\author{
    \IEEEauthorblockN{Fernando Freitas Alves}
    \IEEEauthorblockA{\textit{Center for Engineering, Modeling and Applied Social Sciences}\\
    \textit{Federal University of ABC}\\
    Santo André, Brazil\\
    fernando.freitas@aluno.ufabc.edu.br}
}
\maketitle

%██████████████████████████████████████████████████████████████████████████████████████████████████

\begin{abstract}
    abstract here
\end{abstract}

\begin{IEEEkeywords}
    artificial intelligence, machine learning, reinforcement learning
\end{IEEEkeywords}

%▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒

\section{Introduction}

Reinforcement Learning (RL) is becoming a popular subfield of Machine Learning (ML).
Most ML techniques up to now are data-based rather than on interaction rules-based.
RL is no different.
More specifically, {\Qlearning} is an RL algorithm that gathers environment data and acts against it.
Although invented more than 30 years ago by Watkins \cite{Watkins:1989}.

The idea of RL rely on problems of agent-environment interactions (\reffig{agent-environment}).
An environment is the set of states and laws that dictate how actions change the current state.
The agent, which most of the time is the object of study, performs that action on the environment based on its state.
In turn, the environment replies to the agent with a reward and changes the state.
The agent can also utilize this reward to train itself to achieve a predefined goal, which can be anything from keeping a loop of states or reaching a specific end state.
The precedent events combined in order form what is called the agent-environment loop.

The RL loop described turns this field into a tool to solve problems without analyzing a given environment.
Because the agent does not require prior knowledge of the environment rather than the state (or part of) it can observe, RL algorithms are data-driven.
All the agent needs to care about is to process the observations and rewards from its actions.
A sequence of pairs state-action is defined as a trajectory along with the algorithm steps.
The trajectory is then a sub-product of a policy the agent considers based on the data it gathers.
Hence, there is no need of understanding the physics and laws of the environment for an RL algorithm to solve problems.

\begin{figure}[b]
    \centering
    \includestandalone{img/agent-environment}
    \caption{Agent-environment interaction loop.}
    \label{agent-environment}
\end{figure}

This paper study a specific type of RL algorithm called {\Qlearning}.
Bu as shown a recent real-life application of this algorithm for an online web system auto-configuration \cite{Bu:2009}.
Other works like \cite{Zheng:2018} show that {\Qlearning} is the basis of more complex techniques with better performance, like Deep{\QLearning} where a deep neural network is used to model the {\Qlearning} objective function.
The following sections present how {\Qlearning} works and how to build this algorithm to solve OpenAI Gym \cite{OpenAIGym} environment goals.

%▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒

\section{Method}

{\QLearning} is a learning-process algorithm.
The model is based on an agent, $\agent$, loop interactions with an environment, $\environment$.
In this paper, it is expected that the environment is already implemented.
Thus, the goal is to build the agent to reach a specific goal defined by a problem.

The environment contains all possible states $\states$ as well as tracks its current state $\state \in \states$ within interactions.
This entity is also responsible for reacting against an action $\action \in \actions$ by changing its state and providing a reward value, $\reward$, that reflects an heuristic to the given problem.
The set of rewards, $\rewards$, as well as the state-actions rules are defined by the environment.
However, they are not a necessary information to be passed to the agent.

The agent have only access to the observable states, $\observable \in \observables$ for a given state $\state \in \states \supset \observables$, and the rewards provided by the environment.
Although, the observability of the states depends on the environment and the agent, this paper will treat the observable states and the states as interchangeable, denoting both with $\state$.
So, whenever it is the case that $\observables = \states$ or not, it is implied that the agent will always have access to the observable states and not the states themselves.

The agent entity reacts against the current state with an action $\action$.
The proper action is chosen from a prioritization from all possible actions $\actions$.
In {\Qlearning}, the priorities are defined as $Q$-values.
The best action is then given by the maximum value of a function $\Qf$ quantified for all $\action \in \actions$ in the current state $\state$, as in \ref{best_action}.

\begin{equation}
    \action = \argmax_{\action \in \actions}{\Qf}
    \label{best_action}
\end{equation}

With this model defined, the goal that {\Qlearning} algorithms tackle is find this $Q$ function.

\subsection{Reinforcement learning}\label{reinforcement learning}

\begin{algorithm}[bp]
    \caption{Reinforcement learning general algorithm}
    \begin{algorithmic}[1]
        \FUNCTION  {ReinforcementLearning}[\environment, \agent]
        \INPUT Environment $\environment$, agent $\agent$
        \OUTPUT  Trained agent $\agent$
        \\
        \FORALL {$episodes$}
            \STATE $ \state_0  \gets \function{Reset}(\environment) $
            \STATE $ \state  \gets \state_0 $
            \STATE $ \reward \gets 0 $
            \WHILE {not $\function{Done}(\environment)$}
                \STATE $ \action \gets \function{Act}(\agent, \state) $
                \STATE $ \nextstate, \reward \gets \function{Step}(\environment, \action) $
                \STATE $ \agent \gets \function{Train}(\agent, \state, \action, \nextstate, \reward) $
                \STATE $ \state \gets \nextstate $
            \ENDWHILE
        \ENDFOR
        \RETURN $\agent$
    \end{algorithmic}
    \label{reinforcement learning algorithm}
\end{algorithm}

The environment $\environment$ should have three functions:

\begin{itemize}
    \item $\function{Reset}(\environment)$: a reset function called once each episode. It provides the initial state $\state_0 \in \states$.
    \item $\function{Done}(\environment)$: a done function called at every iteration. It indicates if the simulation is over.
    \item $\function{Step}(\environment, \action)$: a step function called at every iteration. It provides a set of state-reward $(\state\-\reward)$ for a given action $\action$.
\end{itemize}

%░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░

\subsection{\QAgent}\label{qagent}

\begin{algorithm}[ht]
    \caption{{\QAgent} act algorithm}
    \begin{algorithmic}[1]
        \FUNCTION  {Act}[\agent, \state]
        \INPUT      Agent $\agent$, state $\state$
        \PARAMETERS{Agent $\agent$  $\sets$ table of state-action pair values $\Qtarget$, exploration rate $\explorationrate$}
        \OUTPUT     Action $\action$
        \\[1.5pt]
        \STATE $ \Qrandom \gets $ sample $m$ random numbers \\[1.5pt]
        \STATE $ \QftargetNoisy \gets \Qftarget + \explorationrate \Qrandom $ \\[1.5pt]
        \STATE $ \action \gets \argmax_{\action}{\QftargetNoisy} $
        \RETURN $\action$
    \end{algorithmic}
    \label{act}
\end{algorithm}

% Method here \eqref{bellman}.

\begin{equation}
\Qfpolicy = \Expected[\nextstate \sim P]{\rewardf + \discountfactor \Expected[\nextaction \sim \policy]{\nextQfpolicy}}
\label{bellman}
\end{equation}

\begin{equation}
\Qfoptimal = \Expected[\nextstate \sim P]{\rewardf + \discountfactor \max_{\nextaction}{\nextQfoptimal}}
\label{optimal bellman}
\end{equation}

Approximate the optimal action-value function $\Qfoptimal$ by direct small updates of value iteration over a function $\Qftarget$ at time $t$.
According to \cite{Szepesvari:2010}, when $\Qtarget$ is close to $\Qoptimal$, the policy that greedy choose the action with higher $\q$-value will be close to optimal.

\begin{equation}
\qtarget = \rewardf + \discountfactor \max_{\nextaction}{\nextQftarget}
\label{qtarget}
\end{equation}


The {\QLearning} algorithm of Watkins (1989) keeps an estimate $\Qftarget$ of $\Qf$ for each state-action pair $\Qargs \in X \times A$. Upon observing (Xt, At, Rt+1, Yt+1), the estimates are updated as follows:

\begin{algorithm}[ht]
    \caption{Tabular {\Qlearning} training algorithm}
    \begin{algorithmic}[1]
        \FUNCTION  {Train}[\agent, \state, \action, \nextstate, \reward]
        \INPUT      Current state $\state$, action chosen $\action$, next state $\nextstate$, \mbox{reward $\reward$}
        \PARAMETERS{Agent $\agent$  $\sets$ table of state-action pair values $\Qtarget$, discount factor $\discountfactor$, learning rate $\learningrate$}
        \OUTPUT     Agent $\agent$ with updated table of state-action pair values $\nextQtarget$
        \\
        \STATE $ \qtarget \gets \reward + \discountfactor \max_{\nextaction}{\nextQftarget} $
        \STATE $ \error \gets \qtarget - \Qftarget $
        \STATE $ \nextQtarget\Qargs \gets \Qftarget + \learningrate \error $
        \RETURN $\agent$
    \end{algorithmic}
    \label{qlearning}
\end{algorithm}

%▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒

\section{Results}

Results here

\begin{figure*}[!t]
    \centering
    \input{img/results_FrozenLakeNoSlip-v0.pgf}
    \caption{Results from FrozenLakeNoSlip-v0 environment.}
    \label{results_FrozenLakeNoSlip-v0}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \input{img/results_FrozenLake-v0.pgf}
    \caption{Results from FrozenLake-v0 environment.}
    \label{results_FrozenLake-v0}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \input{img/results_MountainCar-v0.pgf}
    \caption{Results from MountainCar-v0 environment.}
    \label{results_MountainCar-v0}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \input{img/results_MountainCarContinuous-v0_3-action-steps.pgf}
    \caption{Results from MountainCarContinuous-v0 with 3 discretized action steps environment.}
    \label{results_MountainCarContinuous-v0-3}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \input{img/results_MountainCarContinuous-v0_20-action-steps.pgf}
    \caption{Results from MountainCarContinuous-v0 with 20 discretized action steps environment.}
    \label{results_MountainCarContinuous-v0-20}
\end{figure*}

%░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░

\subsection{Another subtitle here}\label{AA}
More results here.
% And here Fig.~\ref{fig}.

\begin{itemize}
\item item here.
\end{itemize}

%██████████████████████████████████████████████████████████████████████████████████████████████████

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
