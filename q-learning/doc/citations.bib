@phdthesis{Watkins:1989,
    author   = {Watkins, C. J. C. H.},
    title    = {Learning from Delayed Rewards},
    school   = {King's College, Oxford},
    keywords = {juergen},
    biburl   = {https://www.bibsonomy.org/bibtex/21ffd549077ea1da7675431a17fa2af03/idsia},
    year     = {1989}
}

@inproceedings{Bu:2009,
    abstract  = {In a web system, configuration is crucial to the performance and service availability. It is a challenge, not only because of the dynamics of Internet traffic, but also the dynamic virtual machine environment the system tends to be run on. In this paper, we propose a reinforcement learning approach for autonomic configuration and reconfiguration of multi-tier web systems. It is able to adapt performance parameter settings not only to the change of workload, but also to the change of virtual machine configurations. The RL approach is enhanced with an efficient initialization policy to reduce the learning time for online decision. The approach is evaluated using TPC-W benchmark on a three-tier website hosted on a Xen-based virtual machine environment. Experiment results demonstrate that the approach can auto-configure the web system dynamically in response to the change in both workload and VM resource. It can drive the system into a near-optimal configuration setting in less than 25 trial-and-error iterations. {\textcopyright} 2009 IEEE.},
    author    = {Bu, Xiangping and Rao, Jia and Xu, Cheng Zhong},
    booktitle = {Proceedings - International Conference on Distributed Computing Systems},
    doi       = {10.1109/ICDCS.2009.76},
    isbn      = {9780769536606},
    title     = {{A reinforcement learning approach to online web systems auto-configuration}},
    year      = {2009}
}

@article{Zheng:2018,
    abstract = {In this paper, we propose a novel Deep Reinforcement Learning framework for news recommendation. Online personalized news recommendation is a highly challenging problem due to the dy-namic nature of news features and user preferences. Although some online recommendation models have been proposed to address the dynamic nature of news recommendation, these methods have three major issues. First, they only try to model current reward (e.g., Click Through Rate). Second, very few studies consider to use user feedback other than click / no click labels (e.g., how frequent user returns) to help improve recommendation. Third, these meth-ods tend to keep recommending similar news to users, which may cause users to get bored. Therefore, to address the aforementioned challenges, we propose a Deep Q-Learning based recommendation framework, which can model future reward explicitly. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, an effective exploration strategy is incorporated to find new attrac-tive news for users. Extensive experiments are conducted on the offline dataset and online production environment of a commercial news recommendation application and have shown the superior performance of our methods.},
    author   = {Zheng, Guanjie and Zhang, Fuzheng and Zheng, Zihan and Xiang, Yang and Yuan, Nicholas Jing and Xie, Xing and Li, Zhenhui},
    doi      = {10.1145/3178876.3185994},
    isbn     = {9781450356398},
    journal  = {[WWW2018]Proceedings of the 2018 World Wide Web Conference},
    keywords = {12,24,25,34,45,52,8,Deep Q-Learning,News reco,Reinforcement learning,and hybrid methods,as an extension and,deep learning models,deep q-learning,integration of previous methods,news recommendation,recently,reinforcement learning},
    title    = {{DRN: A Deep Reinforcement Learning Framework for News Recommendation}},
    year     = {2018}
}

@misc{OpenAIGym,
    abstract    = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
    added-at    = {2018-04-12T12:08:39.000+0200},
    author      = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
    biburl      = {https://www.bibsonomy.org/bibtex/2cdc8f927d6c8657ea82951a09e34161a/achakraborty},
    description = {[1606.01540] OpenAI Gym},
    interhash   = {cfd0ba0b44eda9a3ca67480dfbf823a0},
    intrahash   = {cdc8f927d6c8657ea82951a09e34161a},
    keywords    = {2016 arxiv paper reinforcement-learning},
    note        = {cite arxiv:1606.01540},
    timestamp   = {2018-04-12T12:08:39.000+0200},
    title       = {OpenAI Gym},
    url         = {http://arxiv.org/abs/1606.01540},
    year        = {2016}
}

@article{DeepMind:AtariDeepRL,
    author        = {Volodymyr Mnih and
                     Koray Kavukcuoglu and
                     David Silver and
                     Alex Graves and
                     Ioannis Antonoglou and
                     Daan Wierstra and
                     Martin A. Riedmiller},
    title         = {Playing Atari with Deep Reinforcement Learning},
    journal       = {CoRR},
    volume        = {abs/1312.5602},
    url           = {http://arxiv.org/abs/1312.5602},
    archivePrefix = {arXiv},
    eprint        = {1312.5602},
    timestamp     = {Mon, 13 Aug 2018 16:47:42 +0200},
    biburl        = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    year          = {2013}
}

@book{Bellman:DynamicProgramming,
    abstract  = {{An introduction to the mathematical theory of multistage decision processes, this text takes a "functional equation" approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls "a rich lode of applications and research topics." 1957 edition. 37 figures.}},
    added-at  = {2011-08-17T16:08:47.000+0200},
    author    = {Bellman, Richard},
    biburl    = {https://www.bibsonomy.org/bibtex/29cdd821222218ded252c8ba5cd712666/pcbouman},
    interhash = {acf948462171ca060064a7ded257a792},
    intrahash = {9cdd821222218ded252c8ba5cd712666},
    isbn      = {9780486428093},
    keywords  = {book dynamic programming},
    publisher = {Dover Publications},
    timestamp = {2011-08-18T09:10:27.000+0200},
    title     = {{Dynamic Programming}},
    year      = {1957}
}

@inproceedings{Szepesvari:2010,
    author    = {Szepesv{\'{a}}ri, Csaba},
    title     = {{Algorithms for reinforcement learning}},
    keywords  = {Markov Decision Processes,Monte-Carlo methods,PAC-learning,Q-learning,active learning,actor-critic methods,bias-variance tradeoff,function approximation,least-squares methods,natural gradient,online learning,overfitting,planning,policy gradient,reinforcement learning,simulation,simulation optimization,stochastic approximation,stochastic gradient methods,temporal difference learning,two-timescale stochastic approximation},
    doi       = {10.2200/S00268ED1V01Y201005AIM009},
    isbn      = {9781608454921},
    issn      = {19394608},
    year      = {2010}
}

@TechReport{Moore:1990,
    author      = {Moore, Andrew William},
    title       = {{Efficient memory-based learning for robot control}},
    url         = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf},
    institution = {University of Cambridge, Computer Laboratory},
    number      = {UCAM-CL-TR-209},
    month       = nov,
    year        = {1990}
}

@misc{TheComputerScientist:2018,
    author       = {Shawn M.},
    title        = {{Code from 'Intro to OpenAI Gym' tutorial video}},
    publisher    = {GitHub},
    journal      = {GitHub repository},
    howpublished = {\url{https://github.com/the-computer-scientist/OpenAIGym/blob/7be80647e6e\\090c76f28ea03b7d1ba891db75f2f/QLearningIntro.ipynb}},
    month        = oct,
    year         = {2018}
}
