\ProvidesPackage{customcommand}

\usepackage{algorithm,algorithmic}
\usepackage{amsmath}
\usepackage{xparse}

\def\algorithmname{\ALG@name}

\NewDocumentCommand{\reffig}{m}{\figurename~\ref{#1}}
\NewDocumentCommand{\reftable}{m}{\tablename~\ref{#1}}
\NewDocumentCommand{\refalgorithm}{m}{\algorithmname~\ref{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\NewDocumentCommand{\sets}{}{\to}
\RenewDocumentCommand{\-}{}{\textrm{-}}
\NewDocumentCommand{\Real}{}{\mathbb{R}}

\NewDocumentCommand{\state}{}{s}
\NewDocumentCommand{\states}{}{\mathbb{S}}
\NewDocumentCommand{\observable}{}{o}
\NewDocumentCommand{\observables}{}{\mathbb{O}}
\NewDocumentCommand{\action}{}{a}
\NewDocumentCommand{\actions}{}{\mathbb{A}}
\NewDocumentCommand{\explorationrate}{}{\epsilon}
\NewDocumentCommand{\explorationaction}{}{\action_{\explorationrate}}
\NewDocumentCommand{\exploitationaction}{}{\action_{1-\explorationrate}}
\NewDocumentCommand{\policy}{}{\pi}
\NewDocumentCommand{\discountfactor}{}{\gamma}
\NewDocumentCommand{\learningrate}{}{\alpha}
\NewDocumentCommand{\error}{}{e}
\NewDocumentCommand{\environment}{}{\mathcal{E}}
\NewDocumentCommand{\agent}{}{\mathcal{A}}
\NewDocumentCommand{\reward}{}{r}
\NewDocumentCommand{\rewards}{}{\mathbb{R}}
\NewDocumentCommand{\rewardargs}{O{}}{\left(\state#1,\action#1\right)}

\NewDocumentCommand{\q}{O{}}{q#1}
\NewDocumentCommand{\qtarget}{}{\q[_t]}

\NewDocumentCommand{\Q}{O{}}{Q#1}
\NewDocumentCommand{\Qnoisy}{O{}}{\widetilde{Q}#1}
\NewDocumentCommand{\Qpolicy}{}{\Q[^{\pi}]}
\NewDocumentCommand{\Qoptimal}{}{\Q[^{*}]}
\NewDocumentCommand{\Qtarget}{O{}}{\Q[_{t#1}]}
\NewDocumentCommand{\QtargetNoisy}{O{}}{\Qnoisy[_{t#1}]}
\NewDocumentCommand{\Qargs}{O{}}{\left(\state#1,\action#1\right)}
\NewDocumentCommand{\Qrandom}{O{}}{\Qnoisy[#1]}

\NewDocumentCommand{\Qf}{O{}O{}}{\Q[#1]\Qargs[#2]}
\NewDocumentCommand{\Qfpolicy}{O{}}{\Qpolicy\Qargs[#1]}
\NewDocumentCommand{\Qfoptimal}{O{}}{\Qoptimal\Qargs[#1]}
\NewDocumentCommand{\Qftarget}{O{}}{\Qtarget\Qargs[#1]}
\NewDocumentCommand{\rewardf}{O{}}{\reward\rewardargs[#1]}
\NewDocumentCommand{\Expected}{O{}m}{\underset{#1}{\textrm{E}}\left[#2\right]}
\NewDocumentCommand{\QftargetNoisy}{O{}}{\QtargetNoisy\Qargs[#1]}

\NewDocumentCommand{\nextsymbol}{}{^{\prime}}
\NewDocumentCommand{\nextstate}{}{\state\nextsymbol}
\NewDocumentCommand{\nextaction}{}{\action\nextsymbol}
\NewDocumentCommand{\nextQtarget}{}{\Qtarget[+1]}
\NewDocumentCommand{\nextQargs}{}{\Qargs[\nextsymbol]}
\NewDocumentCommand{\nextQf}{O{}}{\Q[#1]\nextQargs}
\NewDocumentCommand{\nextQfpolicy}{}{\Qpolicy\nextQargs}
\NewDocumentCommand{\nextQfoptimal}{}{\Qoptimal\nextQargs}
\NewDocumentCommand{\nextQftarget}{}{\Qtarget\nextQargs}

\NewDocumentCommand{\Qlearning}{}{$\Q$-learning}
\NewDocumentCommand{\QLearning}{}{$\Q$-Learning}
\NewDocumentCommand{\Qagent}{}{$\Q$-agent}
\NewDocumentCommand{\QAgent}{}{$\Q$-Agent}
\NewDocumentCommand{\Qtable}{}{$Q$-table}

\NewDocumentCommand{\function}{}{\textsc}

\NewDocumentCommand{\FUNCTION}{mO{}}{\item[\textbf{function} \function{#1}$\left(#2\right)$]}
\NewDocumentCommand{\PARAMETERS}{}{\item[\textbf{Parameters:}]}
\NewDocumentCommand{\INPUT}{}{\item[\textbf{Input:}]}
\NewDocumentCommand{\OUTPUT}{}{\item[\textbf{Output:}]}
\RenewDocumentCommand{\algorithmiccomment}{O{}}{\hfill\{#1\}\par}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
